{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Ensemble methods**","metadata":{}},{"cell_type":"markdown","source":"# Iris Dataset\n\nIl dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n\n## Caratteristiche e Struttura\n- **Campioni**: 150 campioni di fiori iris.\n- **Features**:\n  - Lunghezza del sepalo (cm)\n  - Larghezza del sepalo (cm)\n  - Lunghezza del petalo (cm)\n  - Larghezza del petalo (cm)\n- **Classi (Etichette Target)**:\n  - *Iris-setosa*\n  - *Iris-versicolor*\n  - *Iris-virginica*\n\nOgni classe è rappresentata da 50 campioni.\n\n## Caratteristiche Principali\n- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n- **Separable Classes**:\n  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n\n\n# Iris Dataset Classes\n\n<table>\n    <tr>\n        <th>Iris Setosa</th>\n        <th>Iris Versicolor</th>\n        <th>Iris Virginica</th>\n    </tr>\n    <tr>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSJqxUtJiLfMX5aIoyPTPz7rMdjxgWagMlBzt0QbfATKzRqH4XnMMDN5aBrU1FvRt19jkHMOrIefjywQlDg9rOeKC6JbA72Wf--jqHD-g\" alt=\"Iris versicolor\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n    </tr>\n</table>\n\n","metadata":{}},{"cell_type":"markdown","source":"# `plot_decision_boundary` Function\n\nLa funzione `plot_decision_boundary` permette di visualizzare i margini decisionali di un classificatore. In questo modo è possibile visualizzare come il modello distingue le feature di campioni assegnati a classi diverse.\n\n1. **Parametri**:\n    - `clf`: Il classificatore allenato che ha il metodo `predict`.\n    - `X`: La matrice dei dati di input, per cui si assume una dimensione 2D per la visualizzazione.\n    - `y`: Le labels corrispondenti ai dati `X`.\n\n2. **Output**:\n    - Un grafico 2D contenente:\n        - **Features**: `Feature 1` e `Feature 2` lungo l' asse x e y (se è stata usata PCA, queste sono le 2 componenti).\n        - **Regioni decisionali**: Colori diversi indicano regioni classificate come labels diverse.\n        - **Camponi**: I punti del dataset (`X`) sono sovrapposti alle regioni e colorati in base alla loro label corretta (`y`).\n\n### **Sintassi**\n\n```python\nplot_decision_boundary(trained_model, X_data, y_data)\n```\n","metadata":{}},{"cell_type":"code","source":"# Helper function to create the plot\n# and visualize the decision boundary\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_decision_boundary(clf, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(\"Decision Boundary Visualization\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:32:54.157981Z","iopub.execute_input":"2025-05-15T18:32:54.158311Z","iopub.status.idle":"2025-05-15T18:32:54.172623Z","shell.execute_reply.started":"2025-05-15T18:32:54.158280Z","shell.execute_reply":"2025-05-15T18:32:54.171278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Funzione `train_test_split()` :\n\n---\n\n## **train_test_split()**\n\nLa funzione `train_test_split()` è parte del modulo `sklearn.model_selection`. Viene utilizzata per dividere un dataset in training e test set.\n\n\n### **Esempio**:\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 1: Classifichiamo il dataset Iris con un DecisionTree**\n\nEseguire tutti gli step di preparazione per l' allenamento di un DecisionTree. Per lo split dai dati in training e test utilizzare:\n\n- `test_size` = `0.3`\n\n- `random_state` = `42`","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# svolgimento...\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Standardizzazione\nscaler = StandardScaler()\ntrain_set= scaler.fit_transform(X_train)\ntest_set = scaler.transform(X_test)\n\n# Inizializzazione e addestramento del modello\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(train_set, y_train)\n\n# Predizione sul test set\ny_pred = clf.predict(test_set)\n\n# Valutazione del modello\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# 7. Visualizzazione dell'albero\nplt.figure(figsize=(12, 8))\nplot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\nplt.title(\"Decision Tree per il dataset Iris\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:32:54.175540Z","iopub.execute_input":"2025-05-15T18:32:54.175946Z","iopub.status.idle":"2025-05-15T18:32:57.393871Z","shell.execute_reply.started":"2025-05-15T18:32:54.175909Z","shell.execute_reply":"2025-05-15T18:32:57.392504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Eesercizio 2: Implementare ensamble methods**\n\nUna volta allenato il DecisionTree nell' esercizio 1, vogliamo applicarvi i metodi di ensamble. Nello specifico andremo a implementare:\n\n* **AdaBoost**\n* **Bagging**\n* **Random Forest**\n\nDi seguito vediamo la sintassi di ognuno di questi.","metadata":{}},{"cell_type":"markdown","source":"## 1. **AdaBoostClassifier**: \n\nL' `AdaBoostClassifier` crea un insieme di alberi decisionali deboli. Assegna un peso a ciascun albero e li combina per formare un modello più robusto.\n\n### Example:\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Train the model\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\nada_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = ada_clf.predict(X_test)\n\n```\n\n---\n\n## 2. **RandomForestClassifier**: \n\nIl `RandomForestClassifier` costruisce più alberi in parallelo e combina i loro risultati per migliorare l'accuratezza.\n\n### Example:\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train the model\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf_clf.predict(X_test)\n\n```\n\n#### **N.B. RandomForest non richiede come argomento il classificatore, differentemente dagli altri metodi.**\n\n---\n\n## 3. **BaggingClassifier**:\n\nIl `BaggingClassifier` combina molteplici modelli base (come DecisionTree) utilizzando la tecnica del bootstrapping per ridurre la varianza.\n\n### Example:\n```python\nfrom sklearn.ensemble import BaggingClassifier\n\n# Train the model\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbag_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = bag_clf.predict(X_test)\n\n```\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n\n# Creare gli oggetti ensemble\n\n# svolgimento...\n# Applica PCA per ridurre a 2 dimensioni (se non lo faccio, mi da errore nella creazione dei grafici)\npca = PCA(n_components=2)\ntrain_pca = pca.fit_transform(train_set)\ntest_pca = pca.transform(test_set)\ntotal_pca = pca.transform(scaler.transform(X))\n\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n\n# Allenare i modelli\n\n# svolgimento...\nada_clf.fit(train_pca, y_train)\nrf_clf.fit(train_pca, y_train)\nbag_clf.fit(train_pca, y_train)\n\n# Valutare e stampare le prestazioni dei modelli\n\n# svolgimento...\npredictions_ada = ada_clf.predict(test_pca)\npredictions_rf = rf_clf.predict(test_pca)\npredictions_bag = bag_clf.predict(test_pca)\n\nprint(\"AdaBoost: \",predictions_ada,\", RandomForest: \",predictions_rf,\", Bagging: \",predictions_bag)\n\nplot_decision_boundary(ada_clf, total_pca, y)\nplot_decision_boundary(rf_clf, total_pca, y)\nplot_decision_boundary(bag_clf, total_pca, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:32:57.394874Z","iopub.execute_input":"2025-05-15T18:32:57.395396Z","iopub.status.idle":"2025-05-15T18:33:01.275633Z","shell.execute_reply.started":"2025-05-15T18:32:57.395355Z","shell.execute_reply":"2025-05-15T18:33:01.274650Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funzioni utili per Ensemble Models\n\n---\n\n## **1. Hard Voting tra tre classificatori**\n\n### **Descrizione**:\nCombina le predizioni da tre classificatori selezionando la casse più votata per ogni campione.\n\n### **Parametri**:\n- `pred1` (numpy array): Predizioni del classificatore 1.\n- `pred2` (numpy array): Predizioni del classificatore 2.\n- `pred3` (numpy array): Predizioni del classificatore 3.\n\n### **Output**:\n- Restituisce un numpy array conenente la classe più votata per ogni campione.\n\n### **Sintassi**:\n```python\nvoted = hard_voting(pred1_test, pred2_test, pred3_test)\nprint(\"Hard Voting Predictions:\", voted)\n```\n\n---\n\n## **2. Allineare predizioni a più classi**\n\n### **Descrizione**:\nAllinea le probabilità in modo da rendere compatibili diversi subsets quando vengono combinati.\n\n### **Parametri**:\n- `pred` (numpy array): Probabilità predette da un classificatore.\n- `classes_present` (list): Classi conosciute dal classificatore.\n- `n_classes` (int, optional): Numero totale di classe, default è 3.\n\n### **Output**:\n- Ritorna un numpy array con le proabilità allineate tra tutte le classi\n- Returns a numpy array with probabilities aligned across all classes.\n\n### **Sintassi**:\n```python\naligned_probs = align_predictions(pred, [0, 1], n_classes=3)\nprint(\"Aligned Probabilities:\", aligned_probs)\n\nMatrice originale [[0.7, 0.3], [0.4, 0.6]]\ndiventa\nMatrice allineata [[0.7, 0, 0.3], [0.4, 0, 0.6]]\n\n```\n\n---\n\n## **3. Plot Decision Boundary**\n\n### **Descrizione**:\nVisualizza i margini decisionali per vari ensamble methods, inclusi `expert1`, `expert2`, `expert3`, `base`, `soft_voting`, `hard_voting`, e `gating`.\n\n### **Parametri**:\n- `X` (numpy array): Dati per la visualizzazione (2D).\n- `y` (numpy array): Lables originali.\n- `clf1`, `clf2`, `clf3` (classifiers, optional): Esperti usati per calcolare prediction.\n- `base` (classifier, optional): Classificatore base.\n- `mode` (string): Metodo di ensamble per la visualizzazione. I valori possibili sono: `\"expert1\"`, `\"expert2\"`, `\"expert3\"`, `\"base\"`, `\"soft_voting\"`, `\"hard_voting\"`, `\"gating\"`.\n\n\n### **Output**:\n- Displays a decision boundary plot for the selected mode.\n\n### **Usage**:\n```python\nplot_decision_boundary(x_test, y_test, expert1=expert1, clf2=expert2, clf3=expert3, base=base_network, mode=\"soft_voting\")\n```\n","metadata":{}},{"cell_type":"markdown","source":"# **Esercizio 3: Implementare Mixture of Experts**\n\nNel seguente esercizio vogliamo implementare un meccanismo di Mixture of Experts. Poichè in iris sono presenti 3 classi, vogliamo allenare 3 classificatori (cioè 3 esperi), rispettivamente:\n\n- esperto 1: riconosce tra la classe 0 e la classe 1.\n- esperto 2: riconosce tra la classe 0 e la classe 2.\n- esperto 3: riconosce tra la classe 1 e la classe 2.\n\nInfine utilizzare la funzione `plot_modes` per plottare le diverse modalità.","metadata":{}},{"cell_type":"code","source":"def hard_voting(pred1, pred2, pred3):\n    combined = np.vstack([pred1, pred2, pred3]).T\n    voted = []\n    for sample in combined:\n        counts = np.bincount(sample)\n        most_common_label = counts.argmax()\n        voted.append(most_common_label)\n    return np.array(voted)\n\ndef align_predictions(pred, classes_present, n_classes=3):\n    aligned = np.zeros((len(pred), n_classes))\n    for idx, class_label in enumerate(classes_present):\n        aligned[:, class_label] = pred[:, idx]\n    return aligned\n\ndef plot_modes(X, y, clf1=None, clf2=None, clf3=None, base=None, mode=\"expert1\"):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    if mode.startswith(\"expert\"):\n        expert_map = {\n            \"expert1\": (clf1, [0, 1]),\n            \"expert2\": (clf2, [0, 2]),\n            \"expert3\": (clf3, [1, 2]),\n        }\n        clf, classes = expert_map[mode]\n        Z = clf.predict(grid).reshape(xx.shape)\n        title = f\"Decision Boundary - {mode.capitalize()}\"\n\n    elif mode == \"base\":\n        Z = base.predict(grid).reshape(xx.shape)\n        title = \"Decision Boundary - Base Network\"\n\n    elif mode == \"soft_voting\":\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = (Z1 + Z2 + Z3).argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Soft Voting\"\n\n    elif mode == \"hard_voting\":\n        Z1 = clf1.predict(grid)\n        Z2 = clf2.predict(grid)\n        Z3 = clf3.predict(grid)\n        Z = np.array([Z1, Z2, Z3])\n        Z = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=Z)\n        Z = Z.reshape(xx.shape)\n        title = \"Decision Boundary - Hard Voting\"\n\n    elif mode == \"gating\":\n        gating_weights = base.predict_proba(grid)\n        gating_weights /= gating_weights.sum(axis=1, keepdims=True)\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = gating_weights * Z1 + gating_weights * Z2 + gating_weights * Z3\n        Z = Z.argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Mixture of Experts (Gating)\"\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:33:01.276898Z","iopub.execute_input":"2025-05-15T18:33:01.277592Z","iopub.status.idle":"2025-05-15T18:33:01.293952Z","shell.execute_reply.started":"2025-05-15T18:33:01.277566Z","shell.execute_reply":"2025-05-15T18:33:01.292853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caricare il dataset Iris\n\n# svolgimento...\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n# Split del dataset in training e test set\n\n# svolgimento...\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Applicare lo scaling e PCA\n\n# svolgimento...\nscaler = StandardScaler()\ntrain_set= scaler.fit_transform(X_train)\ntest_set = scaler.transform(X_test)\n\npca = PCA(n_components=2)\ntrain_pca = pca.fit_transform(train_set)\ntest_pca = pca.transform(test_set)\npca_total = pca.transform(scaler.transform(X))\n\n# Creare 3 modelli di Decision Tree, ognuno dovrà essere esperto in una coppia di classi\n# N.B. ogni esperto dovrà essere allenato su un sottoinsieme contentente solo \n# le classi di competenza\n\n# svolgimento...\ndef create_Decision_Tree(X, y, class_a, class_b):\n    idx = np.where((y == class_a) | (y == class_b))\n    X_sub = X[idx]\n    y_sub = y[idx]\n    # rimappa le etichette in {0,1}\n    y_sub = np.where(y_sub == class_a, 0, 1)\n    return X_sub, y_sub\n\n# Esperto 1: classe 0 vs 1\nX1_train, y1_train = create_Decision_Tree(train_pca, y_train, 0, 1)\nexpert1 = DecisionTreeClassifier(max_depth=3, random_state=42)\nexpert1.fit(X1_train, y1_train)\n\n# Esperto 2: classe 0 vs 2\nX2_train, y2_train = create_Decision_Tree(train_pca, y_train, 0, 2)\nexpert2 = DecisionTreeClassifier(max_depth=3, random_state=42)\nexpert2.fit(X2_train, y2_train)\n\n# Esperto 3: classe 1 vs 2\nX3_train, y3_train = create_Decision_Tree(train_pca, y_train, 1, 2)\nexpert3 = DecisionTreeClassifier(max_depth=3, random_state=42)\nexpert3.fit(X3_train, y3_train)\n\n# Creare un modello di Decision Tree che funge da base network\n# N.B. il base network dovrà essere allenato su tutte le classi, sarà la nostra gating network\n\n# svolgimento...\nbase_network = DecisionTreeClassifier(max_depth=3, random_state=42)\nbase_network.fit(train_pca, y_train)\n\n# Estrarre le predizione di ogni esperto sul test set.\n# N.B. Vogliamo le probabilità di appartenenza a ciascuna classe, quindi useremo `predict_proba`. Inoltre \n# ogni esperto riporterà solo le classi di competenza, quindi dovremo allineare le predizioni.\n\n# svolgimento...\nprob1 = expert1.predict_proba(test_pca)\nprob2 = expert2.predict_proba(test_pca)\nprob3 = expert3.predict_proba(test_pca)\n\n# Allineiamo le probabilità \nP1 = align_predictions(prob1, classes_present=[0,1], n_classes=3)\nP2 = align_predictions(prob2, classes_present=[0,2], n_classes=3)\nP3 = align_predictions(prob3, classes_present=[1,2], n_classes=3)\n\n\n# Estrarre le predizioni del base network sul test set\n\n# svolgimento...\nP_base = base_network.predict_proba(test_pca)\n\n# Usiamo hard voting per combinare le predizioni degli esperti. \n# N.B. hard voting significa che ogni esperto vota per la sua classe di competenza e il voto più comune\n# vince.\n\n# svolgimento...\nhard1 = expert1.predict(test_pca)\nhard2 = expert2.predict(test_pca)\nhard3 = expert3.predict(test_pca)\nhard_votes = hard_voting(hard1, hard2, hard3)\n\n# Usiamo soft voting per combinare le predizioni degli esperti.\n# N.B. soft voting significa che il voto di ogni esperto ha lo stesso peso.\n\n# svolgimento...\nsoft_vote = P1 + P2 + P3\nsoft_votes = soft_vote.argmax(axis=1)\n\n\n# Usiamo gating network per combinare le predizioni degli esperti.\n# N.B. la gating network calcola le probabilità di appartenenza a ciascuna classe e le usa per pesare\n# le predizioni degli esperti.\n\n# svolgimento...\ngating = P_base / P_base.sum(axis=1, keepdims=True)\ngating_scores = gating * P1 + gating * P2 + gating * P3\ngating_votes = gating_scores.argmax(axis=1)\n\nmodes = [\"expert1\", \"expert2\", \"expert3\", \"base\", \"soft_voting\", \"hard_voting\", \"gating\"]\n\n# Plottare le decision boundaries per ogni modalità.\n\n# svolgimento...\nfor mode in modes:\n    plot_modes(\n        np.vstack((train_pca, test_pca)),      \n        np.hstack((y_train, y_test)),            \n        clf1=expert1, clf2=expert2, clf3=expert3,\n        base=base_network,\n        mode=mode\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:34:45.509919Z","iopub.execute_input":"2025-05-15T18:34:45.510216Z","iopub.status.idle":"2025-05-15T18:34:48.314610Z","shell.execute_reply.started":"2025-05-15T18:34:45.510197Z","shell.execute_reply":"2025-05-15T18:34:48.313497Z"}},"outputs":[],"execution_count":null}]}